{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##EX:1"
      ],
      "metadata": {
        "id": "IglzqiTsWsb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " punkt : Includes trained models and resources required for tokenization tasks, such as splitting text into sentences."
      ],
      "metadata": {
        "id": "-jcs7EunVyQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dependencies\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "#Necessary resources for NLTK's Punkt tokenizer.\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from spacy.lang.en import English\n",
        "\n",
        "#load English\n",
        "nlp = English()\n",
        "\n",
        "#load txt file\n",
        "with open(\"doc.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "#Sentences using NLTK\n",
        "sentences_nltk = sent_tokenize(text)\n",
        "\n",
        "print(\"Sentences (NLTK):\")    #print statement\n",
        "for sentence in sentences_nltk:\n",
        "    print(sentence)\n",
        "\n",
        "#Word tokenizer\n",
        "tokens_nltk = word_tokenize(text)\n",
        "\n",
        "print(\"\\nWords (NLTK):\")\n",
        "print(tokens_nltk)\n",
        "\n",
        "#Regular expressions\n",
        "tokens_regex = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "print(\"\\nWords (Regex):\")\n",
        "print(tokens_regex)\n",
        "\n",
        "#SpaCy\n",
        "doc = nlp(text)\n",
        "tokens_spacy = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "print(\"\\nWords (spaCy):\")\n",
        "print(tokens_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3JQ49cmUOlg",
        "outputId": "75dbbadb-5c06-4eb3-9407-b9d564eedcf2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences (NLTK):\n",
            "My name is Meenu Patel.\n",
            "Kindness is not Firtling\n",
            "Attention is not Love\n",
            "Tears are not Weakness\n",
            "Selence is not Anger\n",
            "\n",
            "Words (NLTK):\n",
            "['My', 'name', 'is', 'Meenu', 'Patel', '.', 'Kindness', 'is', 'not', 'Firtling', 'Attention', 'is', 'not', 'Love', 'Tears', 'are', 'not', 'Weakness', 'Selence', 'is', 'not', 'Anger']\n",
            "\n",
            "Words (Regex):\n",
            "['My', 'name', 'is', 'Meenu', 'Patel', 'Kindness', 'is', 'not', 'Firtling', 'Attention', 'is', 'not', 'Love', 'Tears', 'are', 'not', 'Weakness', 'Selence', 'is', 'not', 'Anger']\n",
            "\n",
            "Words (spaCy):\n",
            "['My', 'name', 'is', 'Meenu', 'Patel', '.', 'Kindness', 'is', 'not', 'Firtling', 'Attention', 'is', 'not', 'Love', 'Tears', 'are', 'not', 'Weakness', 'Selence', 'is', 'not', 'Anger']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: POS tagging"
      ],
      "metadata": {
        "id": "OE2BZFPuXelc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using text file"
      ],
      "metadata": {
        "id": "zxzxQOXAcKo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "with open(\"doc.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "#word tokenizer\n",
        "tokens = word_tokenize(text)\n",
        "tokens_regex = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "#POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "tokens_spacy = [token.text for token in doc if not token.is_space]\n",
        "\n",
        "print(\"POS tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "print(\"POS tags with spacy:\")\n",
        "print(tokens_spacy)\n",
        "\n",
        "print(\"POS tags with tokens_regex:\")\n",
        "print(tokens_regex)\n",
        "\n",
        "#For my name: NNP: proper noun, singular (sarah)\n",
        "# VBZ\tverb, present tense with 3rd person singular (bases)\n",
        "# PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
        "# RB\tadverb (occasionally, swiftly)\n",
        "# VBG\tverb gerund (judging)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ6ZStzVYzGn",
        "outputId": "7092e59d-2ebf-4120-c615-33c869410820"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags:\n",
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Meenu', 'NNP'), ('Patel', 'NNP'), ('.', '.'), ('Kindness', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Firtling', 'VBG'), ('Attention', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Love', 'NNP'), ('Tears', 'NNP'), ('are', 'VBP'), ('not', 'RB'), ('Weakness', 'NNP'), ('Selence', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Anger', 'NNP')]\n",
            "POS tags with spacy:\n",
            "['My', 'name', 'is', 'Meenu', 'Patel', '.', 'Kindness', 'is', 'not', 'Firtling', 'Attention', 'is', 'not', 'Love', 'Tears', 'are', 'not', 'Weakness', 'Selence', 'is', 'not', 'Anger']\n",
            "POS tags with tokens_regex:\n",
            "['My', 'name', 'is', 'Meenu', 'Patel', 'Kindness', 'is', 'not', 'Firtling', 'Attention', 'is', 'not', 'Love', 'Tears', 'are', 'not', 'Weakness', 'Selence', 'is', 'not', 'Anger']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#chunck"
      ],
      "metadata": {
        "id": "EA6g9tpjcqAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "with open(\"doc.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "#POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"POS tags:\")\n",
        "print(pos_tags)\n",
        "print()\n",
        "\n",
        "chunk_grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
        "  PP: {<IN><NP>}                # Chunk prepositions followed by NP\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments\n",
        "  CLAUSE: {<NP><VP>}            # Chunk NP, VP pairs\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "parsed_text = chunk_parser.parse(pos_tags)\n",
        "\n",
        "print(\"Chunked tree:\")\n",
        "print(parsed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuW5AKNfYQVw",
        "outputId": "c1467473-87df-4fc3-c8c2-d23533cb7448"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags:\n",
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Meenu', 'NNP'), ('Patel', 'NNP'), ('.', '.'), ('Kindness', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Firtling', 'VBG'), ('Attention', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Love', 'NNP'), ('Tears', 'NNP'), ('are', 'VBP'), ('not', 'RB'), ('Weakness', 'NNP'), ('Selence', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Anger', 'NNP')]\n",
            "\n",
            "Chunked tree:\n",
            "(S\n",
            "  My/PRP$\n",
            "  (NP name/NN)\n",
            "  is/VBZ\n",
            "  (NP Meenu/NNP Patel/NNP)\n",
            "  ./.\n",
            "  (NP Kindness/NNP)\n",
            "  is/VBZ\n",
            "  not/RB\n",
            "  Firtling/VBG\n",
            "  (NP Attention/NNP)\n",
            "  is/VBZ\n",
            "  not/RB\n",
            "  (NP Love/NNP Tears/NNP)\n",
            "  are/VBP\n",
            "  not/RB\n",
            "  (NP Weakness/NNP Selence/NNP)\n",
            "  is/VBZ\n",
            "  not/RB\n",
            "  (NP Anger/NNP))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For the simple sentence in English"
      ],
      "metadata": {
        "id": "X5GnyKfhcvAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "#text file\n",
        "text = \"The quick brown fox jumps over the lazy dog. The cat sits on the mat.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"POS tags:\")\n",
        "print(pos_tags)\n",
        "print()\n",
        "\n",
        "chunk_grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
        "  PP: {<IN><NP>}                # Chunk prepositions followed by NP\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments\n",
        "  CLAUSE: {<NP><VP>}            # Chunk NP, VP pairs\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "#POS tagged text\n",
        "parsed_text = chunk_parser.parse(pos_tags)\n",
        "\n",
        "print(\"Chunked tree:\")\n",
        "print(parsed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeJhlLRGXdci",
        "outputId": "6958b1a3-a370-4aa0-ffb8-eed9a56184c4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags:\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('cat', 'NN'), ('sits', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n",
            "\n",
            "Chunked tree:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN fox/NN)\n",
            "  jumps/VBZ\n",
            "  (PP over/IN (NP the/DT lazy/JJ dog/NN))\n",
            "  ./.\n",
            "  (NP The/DT cat/NN)\n",
            "  sits/VBZ\n",
            "  (PP on/IN (NP the/DT mat/NN))\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dependencies\n",
        "import re\n",
        "\n",
        "#corrections\n",
        "corrections = {\n",
        "    \"teh\": \"the\",\n",
        "    \"wnt\": \"went\",\n",
        "    \"stoe\": \"store\",\n",
        "    \"som\": \"some\",\n",
        "    \"groceres\": \"groceries\"\n",
        "}\n",
        "\n",
        "def apply_corrections(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    corrected_text = text\n",
        "\n",
        "    for word in words:\n",
        "        if word.lower() in corrections:\n",
        "            corrected_word = corrections[word.lower()]\n",
        "            corrected_text = re.sub(r'\\b{}\\b'.format(word), corrected_word, corrected_text)\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "def main():\n",
        "    input_text = \"I wnt too the stoe to by som groceres.\"\n",
        "    print(\"Input Text:\")\n",
        "    print(input_text)\n",
        "\n",
        "    corrected_text = apply_corrections(input_text)\n",
        "    print(\"\\nCorrected Text:\")\n",
        "    print(corrected_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZoFz8PkzaQS",
        "outputId": "4c5c2cad-c080-44b7-c8e3-56b9ce80d2ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:\n",
            "I wnt too the stoe to by som groceres.\n",
            "\n",
            "Corrected Text:\n",
            "I went too the store to by some groceries.\n"
          ]
        }
      ]
    }
  ]
}