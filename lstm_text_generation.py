# -*- coding: utf-8 -*-
"""LSTM_Text_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wNCwCxs2J3XNWsE4VzF0UV8XLmLeBgSl
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import string
# %matplotlib inline

!wget -N "https://cainvas-static.s3.amazonaws.com/media/user_data/cainvas-admin/Speech.txt"
text = ""
with open('Speech.txt', 'r') as f:
    text = f.read()
print(text)

data = text.split("\n")
data[:6]

print("Total lines:",len(data))

data = " ".join(data)
data[:20]

def clean(doc):
    tokens = doc.split()
    table = str.maketrans("","",string.punctuation)
    tokens = [w.translate(table) for w in tokens]
    tokens = [word for word in tokens if word.isalpha()]
    tokens = [word.lower() for word in tokens]
    return tokens

tokens = clean(data)

length = 50 + 1
lines = []
for i in range(length,len(tokens)):
    sequence = tokens[i-length:i]
    line = " ".join(sequence)
    lines.append(line)
print(len(lines))
print(lines[:6])

print("First line:",lines[0])
print("First token:",tokens[0])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM,Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

tokenizer = Tokenizer()
tokenizer.fit_on_texts(lines)

sequences = tokenizer.texts_to_sequences(lines)
sequences = np.array(sequences)

X, y = sequences[:,:-1],sequences[:,-1]

print(X[0])
print(y[0])

vocab_size  = len(tokenizer.word_index) + 1

y = to_categorical(y,num_classes = vocab_size)

model = Sequential()
model.add(Embedding(vocab_size,50,input_length = 50))
model.add(LSTM(100, return_sequences = True))
model.add(LSTM(100))
model.add(Dense(100,activation = "relu"))
model.add(Dense(vocab_size,activation = "softmax"))
model.summary()

model.compile(optimizer ="adam" , loss ="categorical_crossentropy"  ,metrics = ["accuracy"])

record = model.fit(X,y, epochs = 150)

model.save('text_generate.h5')
model = load_model('text_generate.h5')

print(f"loss at epoch 1: {record.history['loss'][0]}")
print(f"loss at epoch 150: {record.history['loss'][149]}")

plt.plot(record.history['loss'])

from tensorflow.keras.models import Sequential

def generate_text_seq(model,tokenizer,text_seq_length,seed_text,n_words):
    if not isinstance(model, Sequential):
        raise TypeError("Model must be a Sequential model")

    encoded = tokenizer.texts_to_sequences([seed_text])[0]
    encoded = pad_sequences([encoded],maxlen = text_seq_length,truncating = 'pre')

    y_predict = model.predict(encoded)[0]
    predicted_words = " "

    for i in range(n_words):
        predicted_word = tokenizer.index_word[y_predict[i]]
        predicted_words += " " + predicted_word

    return predicted_words



def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):
    # Initialize variables
    output_text = []
    input_text = seed_text

    # Generate n_words words
    for i in range(n_words):
        # Convert the input text to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        encoded_text = pad_sequences([encoded_text], maxlen=text_seq_length, truncating='pre')

        # Predict the next word
        y_predict = model.predict(encoded_text)
        predicted_word = tokenizer.index_word[np.argmax(y_predict)]

        # Update the input text with the predicted word
        input_text += " " + predicted_word

        # Add the predicted word to the output text
        output_text.append(predicted_word)

    # Return the generated text
    return " ".join(output_text)

generate_text_seq(model,tokenizer,50,seed_text,100)