# -*- coding: utf-8 -*-
"""NLP_EXP_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VI6lOpQtKqfAKAcwCYYg51c-foEDUeLs

#*Removal* of stop words using NLTK

EXP: 4

###First applying this method with the text file to analyze the large size of the file like text doc.
###Here, i've applied this method on my research paper data for the further analysis we can also increase the size of the file.
"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')

#declaring the function to remove stop wordsn for example: is, am and many more
def remove_stopwords(words):
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return filtered_words

#declaring the funcition to read text from a text.txt file
def read_text(filename):
    with open(filename, 'r') as file:
        text = file.read()
    return text

#defining Main function
def main():
    #text from file
    filename = 'text.txt'  # Change to the name of your .txt file
    text = read_text(filename)

    #tokenize the text into sentences
    sentences = sent_tokenize(text)

    #tokenize each sentence into words,
    #remove stop words, and then rejoining into sentences
    cleaned_sentences = []
    for sentence in sentences:
        words = word_tokenize(sentence)
        cleaned_words = remove_stopwords(words)
        cleaned_sentence = ' '.join(cleaned_words)
        cleaned_sentences.append(cleaned_sentence)

    #Result
    print("Cleaned sentences:")
    for sentence in cleaned_sentences:
        print(sentence)

if __name__ == "__main__":
    main()

"""Performing stopword method on sentences
Basicaly it's for the small size of the text data.
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

#NLTK stopwords data
nltk.download('stopwords')
nltk.download('punkt')

text = "Natural language processing is a machine learning technology"

#tokenize the text into words
words = word_tokenize(text)

#Remove stop words
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.lower() not in stop_words]

# Print the filtered words
print("Original text:", text)
print("\nFiltered words:")
print(filtered_words)

#result
#is, a both are removed
