# -*- coding: utf-8 -*-
"""NLP_5th.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c1LsNYtub_qJexBnF21pRXfpwPNxCV0_
"""



"""#Movie Review

####Frequency of the word occurring in dataset (Sentence or large text file)
"""

#dependencies
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
nltk.download('punkt')

import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
nltk.download('punkt')
def word_frequency_from_file(file_path, target_word):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()

    tokens = word_tokenize(text)
    tokens = [token.lower() for token in tokens]

    word_counts = Counter(tokens)
    return word_counts[target_word]
#defining the function for the word tokenization
def word_frequency_from_sentences(sentences, target_word):
    tokens = []
    for sentence in sentences:
        tokens.extend(word_tokenize(sentence))
    tokens = [token.lower() for token in tokens]

    word_counts = Counter(tokens)
    return word_counts[target_word]
    #here we are trying to find with the txt file
file_path = 'text.txt'
word_to_find = 'a'
freq_in_file = word_frequency_from_file(file_path, word_to_find)
print(f"The word '{word_to_find}' appears {freq_in_file} times in the file.")
#with the help of the sentence file
sentences = ["You have to apply different Tokenization"]
word_to_find = 'to'
freq_in_sentences = word_frequency_from_sentences(sentences, word_to_find)
print(f"The word '{word_to_find}' appears {freq_in_sentences} times in the sentences.")

"""Sentence"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

def word_frequency_with_nltk(file_path):
    with open(file_path, 'r') as file:
        text = file.read()

    words = word_tokenize(text)

    frequency_distribution = FreqDist(words)
    return frequency_distribution

file_path = 'text.txt'
word_freq_dist = word_frequency_with_nltk(file_path)

for word, freq in word_freq_dist.items():
    print(f"{word}: {freq}")

import nltk
from nltk.corpus import stopwords  #stop word method
#without tokenizarion
from nltk.probability import FreqDist

def word_frequency_without_tokenization(file_path, target_word):
    with open(file_path, 'r') as file:
        text = file.read()

    text_lower = text.lower()

    lines = text_lower.split('\n')

    frequency_distribution = FreqDist()

    for line in lines:
        words = line.split()

        frequency_distribution.update([word for word in words if word == target_word.lower()])

    return frequency_distribution[target_word.lower()]

file_path = 'text.txt'
word_to_find = 'good'
freq = word_frequency_without_tokenization(file_path, word_to_find)
print(f"The word '{word_to_find}' appears {freq} times in the text.")

import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from collections import Counter

# Download NLTK resources
nltk.download('punkt')

# Function to read and preprocess text from a file
def preprocess_text(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()

    # Tokenize words
    tokens = word_tokenize(text)
    # Convert tokens to lowercase
    tokens = [token.lower() for token in tokens]
    return ' '.join(tokens)

# Example file paths
file_paths = ['text1.txt', 'text2.txt']
# Labels for each file
labels = ['class1', 'class2']

# Preprocess texts and create corpus
corpus = [preprocess_text(file_path) for file_path in file_paths]

# TF-IDF Vectorization
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

# SVM Classifier
svm_classifier = SVC(kernel='linear')
svm_classifier.fit(X_train, y_train)

# Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Example prediction using SVM
example_text = preprocess_text('example_text.txt')
example_text_vectorized = vectorizer.transform([example_text])
svm_prediction = svm_classifier.predict(example_text_vectorized)
print("SVM Prediction:", svm_prediction[0])

# Example prediction using Random Forest
rf_prediction = rf_classifier.predict(example_text_vectorized)
print("Random Forest Prediction:", rf_prediction[0])
