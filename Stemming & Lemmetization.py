# -*- coding: utf-8 -*-
"""NPL_EXP_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UH0VXfx4eXIS40U11x-utWpAj5eKTqI0

#EXP: 2

punkt : Includes trained models and resources required for tokenization tasks, such as splitting text into sentences.

##Importing dependencies
"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer
from nltk import pos_tag

#Read the content of the file
with open("doc.txt", "r") as file:
    text = file.read()

#Tokenize the text into words using NLTK word tokenizer
tokens_word = word_tokenize(text)

#Tokenize the text into sentences using NLTK sentence tokenizer
sentences = sent_tokenize(text)

#Tokenize the text into words using NLTK regex tokenizer
tokenizer = RegexpTokenizer(r'\w+')
tokens_regex = tokenizer.tokenize(text)

#Perform POS tagging for NLTK word tokenizer
pos_tags_word = pos_tag(tokens_word)

#Perform POS tagging for NLTK sentence tokenizer
pos_tags_sentence = [pos_tag(word_tokenize(sentence)) for sentence in sentences]

#Perform POS tagging for NLTK regex tokenizer
pos_tags_regex = pos_tag(tokens_regex)

print("POS tags (Word Tokenizer):")
print(pos_tags_word)
print()

print("POS tags (Sentence Tokenizer):")
for i, sentence_pos_tags in enumerate(pos_tags_sentence):
    print(f"Sentence {i+1}: {sentence_pos_tags}")
print()

print("POS tags (Regex Tokenizer):")
print(pos_tags_regex)

#“Access to the Vedas is the greatest privilege this century may claim over all previous centuries. ― J. Robert Oppenheimer

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer
from nltk import pos_tag

# Sample text
text = "“       Access to the Vedas is the greatest privilege this century may claim over all previous centuries. ― J. Robert Oppenheimer”"

# Tokenize the text into words using NLTK word tokenizer
tokens_word = word_tokenize(text)

# Tokenize the text into sentences using NLTK sentence tokenizer
sentences = sent_tokenize(text)

# Tokenize the text into words using NLTK regex tokenizer
tokenizer = RegexpTokenizer(r'\w+')
tokens_regex = tokenizer.tokenize(text)

# Perform POS tagging for NLTK word tokenizer
pos_tags_word = pos_tag(tokens_word)

# Perform POS tagging for NLTK sentence tokenizer
pos_tags_sentence = [pos_tag(word_tokenize(sentence)) for sentence in sentences]

# Perform POS tagging for NLTK regex tokenizer
pos_tags_regex = pos_tag(tokens_regex)

print("POS tags (Word Tokenizer):")
print(pos_tags_word)
print()

print("POS tags (Sentence Tokenizer):")
for i, sentence_pos_tags in enumerate(pos_tags_sentence):
    print(f"Sentence {i+1}: {sentence_pos_tags}")
print()

print("POS tags (Regex Tokenizer):")
print(pos_tags_regex)
