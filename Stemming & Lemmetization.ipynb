{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EXP: 2"
      ],
      "metadata": {
        "id": "IglzqiTsWsb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " punkt : Includes trained models and resources required for tokenization tasks, such as splitting text into sentences."
      ],
      "metadata": {
        "id": "-jcs7EunVyQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing dependencies"
      ],
      "metadata": {
        "id": "GCBvzfIfJK-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "#Read the content of the file\n",
        "with open(\"doc.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "#Tokenize the text into words using NLTK word tokenizer\n",
        "tokens_word = word_tokenize(text)\n",
        "\n",
        "#Tokenize the text into sentences using NLTK sentence tokenizer\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "#Tokenize the text into words using NLTK regex tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens_regex = tokenizer.tokenize(text)\n",
        "\n",
        "#Perform POS tagging for NLTK word tokenizer\n",
        "pos_tags_word = pos_tag(tokens_word)\n",
        "\n",
        "#Perform POS tagging for NLTK sentence tokenizer\n",
        "pos_tags_sentence = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "#Perform POS tagging for NLTK regex tokenizer\n",
        "pos_tags_regex = pos_tag(tokens_regex)\n",
        "\n",
        "print(\"POS tags (Word Tokenizer):\")\n",
        "print(pos_tags_word)\n",
        "print()\n",
        "\n",
        "print(\"POS tags (Sentence Tokenizer):\")\n",
        "for i, sentence_pos_tags in enumerate(pos_tags_sentence):\n",
        "    print(f\"Sentence {i+1}: {sentence_pos_tags}\")\n",
        "print()\n",
        "\n",
        "print(\"POS tags (Regex Tokenizer):\")\n",
        "print(pos_tags_regex)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3JQ49cmUOlg",
        "outputId": "d014c236-ad23-4197-bd51-d4f40d8ccb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags (Word Tokenizer):\n",
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Meenu', 'NNP'), ('Patel', 'NNP'), ('.', '.'), ('Kindness', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Firtling', 'VBG'), ('Attention', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Love', 'NNP'), ('Tears', 'NNP'), ('are', 'VBP'), ('not', 'RB'), ('Weakness', 'NNP'), ('Selence', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Anger', 'NNP')]\n",
            "\n",
            "POS tags (Sentence Tokenizer):\n",
            "Sentence 1: [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Meenu', 'NNP'), ('Patel', 'NNP'), ('.', '.')]\n",
            "Sentence 2: [('Kindness', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Firtling', 'VBG'), ('Attention', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Love', 'NNP'), ('Tears', 'NNP'), ('are', 'VBP'), ('not', 'RB'), ('Weakness', 'NNP'), ('Selence', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Anger', 'NNP')]\n",
            "\n",
            "POS tags (Regex Tokenizer):\n",
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Meenu', 'NNP'), ('Patel', 'NNP'), ('Kindness', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Firtling', 'VBG'), ('Attention', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Love', 'NNP'), ('Tears', 'NNP'), ('are', 'VBP'), ('not', 'RB'), ('Weakness', 'NNP'), ('Selence', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('Anger', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#“Access to the Vedas is the greatest privilege this century may claim over all previous centuries. ― J. Robert Oppenheimer"
      ],
      "metadata": {
        "id": "C8La9uBtU7qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sample text\n",
        "text = \"“       Access to the Vedas is the greatest privilege this century may claim over all previous centuries. ― J. Robert Oppenheimer”\"\n",
        "\n",
        "# Tokenize the text into words using NLTK word tokenizer\n",
        "tokens_word = word_tokenize(text)\n",
        "\n",
        "# Tokenize the text into sentences using NLTK sentence tokenizer\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Tokenize the text into words using NLTK regex tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens_regex = tokenizer.tokenize(text)\n",
        "\n",
        "# Perform POS tagging for NLTK word tokenizer\n",
        "pos_tags_word = pos_tag(tokens_word)\n",
        "\n",
        "# Perform POS tagging for NLTK sentence tokenizer\n",
        "pos_tags_sentence = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Perform POS tagging for NLTK regex tokenizer\n",
        "pos_tags_regex = pos_tag(tokens_regex)\n",
        "\n",
        "print(\"POS tags (Word Tokenizer):\")\n",
        "print(pos_tags_word)\n",
        "print()\n",
        "\n",
        "print(\"POS tags (Sentence Tokenizer):\")\n",
        "for i, sentence_pos_tags in enumerate(pos_tags_sentence):\n",
        "    print(f\"Sentence {i+1}: {sentence_pos_tags}\")\n",
        "print()\n",
        "\n",
        "print(\"POS tags (Regex Tokenizer):\")\n",
        "print(pos_tags_regex)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9iVxjcHVAHe",
        "outputId": "b6f17571-0266-4011-cf52-c2763040d434"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags (Word Tokenizer):\n",
            "[('“', 'JJ'), ('Access', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('Vedas', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('greatest', 'JJS'), ('privilege', 'NN'), ('this', 'DT'), ('century', 'NN'), ('may', 'MD'), ('claim', 'VB'), ('over', 'IN'), ('all', 'DT'), ('previous', 'JJ'), ('centuries', 'NNS'), ('.', '.'), ('―', 'NNP'), ('J.', 'NNP'), ('Robert', 'NNP'), ('Oppenheimer', 'NNP'), ('@', 'NNP'), ('#', '#'), ('”', 'NN')]\n",
            "\n",
            "POS tags (Sentence Tokenizer):\n",
            "Sentence 1: [('“', 'JJ'), ('Access', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('Vedas', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('greatest', 'JJS'), ('privilege', 'NN'), ('this', 'DT'), ('century', 'NN'), ('may', 'MD'), ('claim', 'VB'), ('over', 'IN'), ('all', 'DT'), ('previous', 'JJ'), ('centuries', 'NNS'), ('.', '.')]\n",
            "Sentence 2: [('―', 'NNP'), ('J.', 'NNP'), ('Robert', 'NNP'), ('Oppenheimer', 'NNP'), ('@', 'NNP'), ('#', '#'), ('”', 'NN')]\n",
            "\n",
            "POS tags (Regex Tokenizer):\n",
            "[('Access', 'NN'), ('to', 'TO'), ('the', 'DT'), ('Vedas', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('greatest', 'JJS'), ('privilege', 'NN'), ('this', 'DT'), ('century', 'NN'), ('may', 'MD'), ('claim', 'VB'), ('over', 'IN'), ('all', 'DT'), ('previous', 'JJ'), ('centuries', 'NNS'), ('J', 'NNP'), ('Robert', 'NNP'), ('Oppenheimer', 'NNP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}
