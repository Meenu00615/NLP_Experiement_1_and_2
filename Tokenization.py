# -*- coding: utf-8 -*-
"""NPL_EXP_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gfvQ2TEosnlm-aWtUrr73D9RZr4tISlx

##EX:1

punkt : Includes trained models and resources required for tokenization tasks, such as splitting text into sentences.
"""

#importing dependencies
import nltk
import re
import spacy

#Necessary resources for NLTK's Punkt tokenizer.
nltk.download('punkt')

from nltk.tokenize import sent_tokenize, word_tokenize
from spacy.lang.en import English

#load English
nlp = English()

#load txt file
with open("doc.txt", "r") as file:
    text = file.read()

#Sentences using NLTK
sentences_nltk = sent_tokenize(text)

print("Sentences (NLTK):")    #print statement
for sentence in sentences_nltk:
    print(sentence)

#Word tokenizer
tokens_nltk = word_tokenize(text)

print("\nWords (NLTK):")
print(tokens_nltk)

#Regular expressions
tokens_regex = re.findall(r'\b\w+\b', text)

print("\nWords (Regex):")
print(tokens_regex)

#SpaCy
doc = nlp(text)
tokens_spacy = [token.text for token in doc if not token.is_space]

print("\nWords (spaCy):")
print(tokens_spacy)
