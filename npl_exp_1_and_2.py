# -*- coding: utf-8 -*-
"""NPL_EXP_1_and_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gfvQ2TEosnlm-aWtUrr73D9RZr4tISlx

##EX:1

punkt : Includes trained models and resources required for tokenization tasks, such as splitting text into sentences.
"""

#importing dependencies
import nltk
import re
import spacy

#Necessary resources for NLTK's Punkt tokenizer.
nltk.download('punkt')

from nltk.tokenize import sent_tokenize, word_tokenize
from spacy.lang.en import English

#load English
nlp = English()

#load txt file
with open("doc.txt", "r") as file:
    text = file.read()

#Sentences using NLTK
sentences_nltk = sent_tokenize(text)

print("Sentences (NLTK):")    #print statement
for sentence in sentences_nltk:
    print(sentence)

#Word tokenizer
tokens_nltk = word_tokenize(text)

print("\nWords (NLTK):")
print(tokens_nltk)

#Regular expressions
tokens_regex = re.findall(r'\b\w+\b', text)

print("\nWords (Regex):")
print(tokens_regex)

#SpaCy
doc = nlp(text)
tokens_spacy = [token.text for token in doc if not token.is_space]

print("\nWords (spaCy):")
print(tokens_spacy)

"""## Experiment 2: POS tagging

Using text file
"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk import pos_tag

with open("doc.txt", "r") as file:
    text = file.read()
#word tokenizer
tokens = word_tokenize(text)
tokens_regex = re.findall(r'\b\w+\b', text)

#POS tagging
pos_tags = pos_tag(tokens)
tokens_spacy = [token.text for token in doc if not token.is_space]

print("POS tags:")
print(pos_tags)

print("POS tags with spacy:")
print(tokens_spacy)

print("POS tags with tokens_regex:")
print(tokens_regex)

#For my name: NNP: proper noun, singular (sarah)
# VBZ	verb, present tense with 3rd person singular (bases)
# PRP$	possessive pronoun (her, his, mine, my, our )
# RB	adverb (occasionally, swiftly)
# VBG	verb gerund (judging)
#

"""#chunck"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.chunk import RegexpParser

with open("doc.txt", "r") as file:
    text = file.read()

tokens = word_tokenize(text)

#POS tagging
pos_tags = pos_tag(tokens)

print("POS tags:")
print(pos_tags)
print()

chunk_grammar = r"""
  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN
  PP: {<IN><NP>}                # Chunk prepositions followed by NP
  VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments
  CLAUSE: {<NP><VP>}            # Chunk NP, VP pairs
"""

chunk_parser = RegexpParser(chunk_grammar)

parsed_text = chunk_parser.parse(pos_tags)

print("Chunked tree:")
print(parsed_text)

"""#For the simple sentence in English"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.chunk import RegexpParser

#text file
text = "The quick brown fox jumps over the lazy dog. The cat sits on the mat."

tokens = word_tokenize(text)

pos_tags = pos_tag(tokens)

print("POS tags:")
print(pos_tags)
print()

chunk_grammar = r"""
  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN
  PP: {<IN><NP>}                # Chunk prepositions followed by NP
  VP: {<VB.*><NP|PP|CLAUSE>+$}  # Chunk verbs and their arguments
  CLAUSE: {<NP><VP>}            # Chunk NP, VP pairs
"""

chunk_parser = RegexpParser(chunk_grammar)

#POS tagged text
parsed_text = chunk_parser.parse(pos_tags)

print("Chunked tree:")
print(parsed_text)

#importing dependencies
import re

#corrections
corrections = {
    "teh": "the",
    "wnt": "went",
    "stoe": "store",
    "som": "some",
    "groceres": "groceries"
}

def apply_corrections(text):
    words = re.findall(r'\b\w+\b', text)
    corrected_text = text

    for word in words:
        if word.lower() in corrections:
            corrected_word = corrections[word.lower()]
            corrected_text = re.sub(r'\b{}\b'.format(word), corrected_word, corrected_text)

    return corrected_text

def main():
    input_text = "I wnt too the stoe to by som groceres."
    print("Input Text:")
    print(input_text)

    corrected_text = apply_corrections(input_text)
    print("\nCorrected Text:")
    print(corrected_text)

if __name__ == "__main__":
    main()