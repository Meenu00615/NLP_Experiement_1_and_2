{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1-z1zTyeTWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iq4UHWuHeTGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Movie Review"
      ],
      "metadata": {
        "id": "SiFT2uLckaT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Frequency of the word occurring in dataset (Sentence or large text file)"
      ],
      "metadata": {
        "id": "wvibirEuiOyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dependencies\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGFfCe00eT_t",
        "outputId": "96d6cc91-d528-4af5-fec2-58153b263e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DKDpVNNc215",
        "outputId": "1196ef65-a7a5-4602-8fbd-2f49dfcc2491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'a' appears 65 times in the file.\n",
            "The word 'to' appears 1 times in the sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "def word_frequency_from_file(file_path, target_word):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    word_counts = Counter(tokens)\n",
        "    return word_counts[target_word]\n",
        "#defining the function for the word tokenization\n",
        "def word_frequency_from_sentences(sentences, target_word):\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        tokens.extend(word_tokenize(sentence))\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    word_counts = Counter(tokens)\n",
        "    return word_counts[target_word]\n",
        "    #here we are trying to find with the txt file\n",
        "file_path = 'text.txt'\n",
        "word_to_find = 'a'\n",
        "freq_in_file = word_frequency_from_file(file_path, word_to_find)\n",
        "print(f\"The word '{word_to_find}' appears {freq_in_file} times in the file.\")\n",
        "#with the help of the sentence file\n",
        "sentences = [\"You have to apply different Tokenization\"]\n",
        "word_to_find = 'to'\n",
        "freq_in_sentences = word_frequency_from_sentences(sentences, word_to_find)\n",
        "print(f\"The word '{word_to_find}' appears {freq_in_sentences} times in the sentences.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence"
      ],
      "metadata": {
        "id": "IlhLIS-JdS5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def word_frequency_with_nltk(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    frequency_distribution = FreqDist(words)\n",
        "    return frequency_distribution\n",
        "\n",
        "file_path = 'text.txt'\n",
        "word_freq_dist = word_frequency_with_nltk(file_path)\n",
        "\n",
        "for word, freq in word_freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeHaISkPdRh-",
        "outputId": "87689d1c-0ce0-4ad3-be98-68deb092a1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One: 3\n",
            "of: 53\n",
            "the: 82\n",
            "other: 1\n",
            "reviewers: 1\n",
            "has: 9\n",
            "mentioned: 2\n",
            "that: 15\n",
            "after: 1\n",
            "watching: 2\n",
            "just: 6\n",
            "1: 1\n",
            "Oz: 1\n",
            "episode: 1\n",
            "you: 9\n",
            "'ll: 1\n",
            "be: 11\n",
            "hooked: 1\n",
            ".: 46\n",
            "The: 20\n",
            "...: 101\n",
            "positive: 43\n",
            "A: 5\n",
            "wonderful: 2\n",
            "little: 5\n",
            "production: 2\n",
            "<: 18\n",
            "br: 17\n",
            "/: 16\n",
            ">: 16\n",
            "filming: 1\n",
            "technique: 1\n",
            "is: 36\n",
            "very: 4\n",
            "unassuming-: 1\n",
            "old-time-B: 1\n",
            "I: 55\n",
            "thought: 3\n",
            "this: 33\n",
            "was: 22\n",
            "a: 60\n",
            "way: 2\n",
            "to: 37\n",
            "spend: 1\n",
            "time: 1\n",
            "on: 6\n",
            "too: 3\n",
            "hot: 1\n",
            "summer: 1\n",
            "weekend: 1\n",
            ",: 80\n",
            "sitting: 2\n",
            "in: 32\n",
            "air: 1\n",
            "con: 2\n",
            "Basically: 1\n",
            "there: 3\n",
            "'s: 29\n",
            "family: 1\n",
            "where: 2\n",
            "boy: 1\n",
            "(: 10\n",
            "Jake: 1\n",
            "): 11\n",
            "thinks: 1\n",
            "zombie: 1\n",
            "his: 3\n",
            "closet: 1\n",
            "&: 3\n",
            "par: 2\n",
            "negative: 58\n",
            "Petter: 1\n",
            "Mattei: 2\n",
            "``: 6\n",
            "Love: 1\n",
            "Time: 1\n",
            "Money: 1\n",
            "'': 13\n",
            "visually: 1\n",
            "stunning: 2\n",
            "film: 19\n",
            "watch: 3\n",
            "Mr.: 1\n",
            "offers: 1\n",
            "Probably: 1\n",
            "my: 5\n",
            "all-time: 1\n",
            "favorite: 1\n",
            "movie: 27\n",
            "story: 4\n",
            "selflessness: 1\n",
            "sacrifice: 1\n",
            "and: 36\n",
            "dedication: 1\n",
            "noble: 1\n",
            "ca: 1\n",
            "sure: 2\n",
            "would: 5\n",
            "like: 7\n",
            "see: 7\n",
            "resurrection: 1\n",
            "up: 3\n",
            "dated: 1\n",
            "Seahunt: 1\n",
            "series: 3\n",
            "with: 11\n",
            "tech: 1\n",
            "they: 3\n",
            "have: 17\n",
            "today: 1\n",
            "i: 5\n",
            "This: 14\n",
            "show: 1\n",
            "an: 8\n",
            "amazing: 1\n",
            "fresh: 1\n",
            "innovative: 1\n",
            "idea: 2\n",
            "70: 2\n",
            "when: 5\n",
            "it: 28\n",
            "first: 6\n",
            "aired: 1\n",
            "7: 1\n",
            "or: 5\n",
            "8: 1\n",
            "Encouraged: 1\n",
            "by: 11\n",
            "comments: 1\n",
            "about: 10\n",
            "here: 4\n",
            "looking: 1\n",
            "forward: 1\n",
            "f: 2\n",
            "If: 5\n",
            "original: 3\n",
            "gut: 1\n",
            "wrenching: 1\n",
            "laughter: 1\n",
            "will: 2\n",
            "are: 3\n",
            "young: 2\n",
            "old: 1\n",
            "then: 3\n",
            "y: 2\n",
            "Phil: 1\n",
            "Alien: 1\n",
            "one: 15\n",
            "those: 2\n",
            "quirky: 1\n",
            "films: 5\n",
            "humour: 1\n",
            "based: 3\n",
            "around: 1\n",
            "oddness: 1\n",
            "everythi: 1\n",
            "saw: 3\n",
            "12: 1\n",
            "came: 3\n",
            "out: 4\n",
            "recall: 1\n",
            "scariest: 1\n",
            "scene: 3\n",
            "big: 2\n",
            "bird: 1\n",
            "So: 2\n",
            "im: 2\n",
            "not: 13\n",
            "fan: 3\n",
            "Boll: 1\n",
            "work: 1\n",
            "but: 5\n",
            "again: 2\n",
            "many: 5\n",
            "enjoyed: 2\n",
            "Postal: 1\n",
            "maybe: 1\n",
            "cast: 2\n",
            "played: 1\n",
            "Shakespeare.: 1\n",
            "Shakespeare: 1\n",
            "lost.: 1\n",
            "appreciate: 1\n",
            "tryin: 1\n",
            "fantastic: 1\n",
            "three: 1\n",
            "prisoners: 1\n",
            "who: 5\n",
            "become: 1\n",
            "famous: 1\n",
            "actors: 1\n",
            "george: 1\n",
            "clooney: 1\n",
            "Kind: 1\n",
            "drawn: 1\n",
            "erotic: 1\n",
            "scenes: 3\n",
            "only: 7\n",
            "realize: 1\n",
            "most: 5\n",
            "amateurish: 1\n",
            "unbel: 1\n",
            "Some: 1\n",
            "simply: 1\n",
            "should: 1\n",
            "remade: 1\n",
            "them: 1\n",
            "In: 1\n",
            "itself: 3\n",
            "bad: 3\n",
            "made: 3\n",
            "into: 2\n",
            "top: 1\n",
            "10: 1\n",
            "awful: 2\n",
            "movies: 7\n",
            "Horrible: 1\n",
            "There: 2\n",
            "n't: 6\n",
            "remember: 4\n",
            "had: 5\n",
            "watched: 6\n",
            "at: 7\n",
            "cinema: 1\n",
            "picture: 1\n",
            "dark: 2\n",
            "place: 1\n",
            "An: 2\n",
            "!: 7\n",
            "It: 8\n",
            "must: 2\n",
            "been: 8\n",
            "against: 1\n",
            "some: 4\n",
            "real: 2\n",
            "stinkers: 1\n",
            "nominated: 1\n",
            "for: 8\n",
            "Golden: 1\n",
            "Globe: 1\n",
            "....: 1\n",
            "After: 3\n",
            "success: 1\n",
            "Die: 1\n",
            "Hard: 2\n",
            "sequels: 1\n",
            "no: 5\n",
            "surprise: 2\n",
            "really: 5\n",
            "1990s: 1\n",
            "glut: 1\n",
            "terrible: 1\n",
            "misfortune: 1\n",
            "having: 2\n",
            "view: 1\n",
            "b-movie: 1\n",
            "entirety.: 1\n",
            "All: 2\n",
            "ha: 1\n",
            "What: 3\n",
            "absolutely: 1\n",
            "if: 2\n",
            "2.5: 1\n",
            "hrs: 1\n",
            "kill: 1\n",
            "wo: 1\n",
            "regret: 1\n",
            "First: 1\n",
            "all: 6\n",
            "let: 2\n",
            "get: 1\n",
            "few: 2\n",
            "things: 3\n",
            "straight: 1\n",
            ":: 2\n",
            "AM: 1\n",
            "anime: 1\n",
            "fan-: 1\n",
            "always: 1\n",
            "as: 15\n",
            "matte: 1\n",
            "worst: 6\n",
            "WorldFest: 1\n",
            "also: 2\n",
            "received: 1\n",
            "least: 3\n",
            "amount: 1\n",
            "applause: 1\n",
            "afterw: 1\n",
            "Karen: 2\n",
            "Carpenter: 2\n",
            "Story: 1\n",
            "shows: 1\n",
            "more: 2\n",
            "singer: 1\n",
            "complex: 1\n",
            "life: 3\n",
            "Though: 1\n",
            "Cell: 1\n",
            "exotic: 1\n",
            "masterpiece: 1\n",
            "dizzying: 1\n",
            "trip: 1\n",
            "vast: 1\n",
            "mind: 2\n",
            "serial: 1\n",
            "killer: 1\n",
            "tried: 2\n",
            "once: 2\n",
            "stinging: 1\n",
            "political: 1\n",
            "satire: 1\n",
            "Hollywood: 3\n",
            "blockbuster: 1\n",
            "so: 5\n",
            "frustrating: 1\n",
            "Everything: 1\n",
            "seemed: 1\n",
            "energetic: 1\n",
            "totally: 1\n",
            "prepared: 1\n",
            "good: 4\n",
            "'War: 1\n",
            "': 3\n",
            "genre: 1\n",
            "done: 1\n",
            "redone: 1\n",
            "times: 2\n",
            "clichéd: 1\n",
            "dialogue: 1\n",
            "Taut: 1\n",
            "organically: 1\n",
            "gripping: 1\n",
            "Edward: 1\n",
            "Dmytryk: 1\n",
            "Crossfire: 1\n",
            "distinctive: 1\n",
            "suspense: 1\n",
            "thriller: 1\n",
            "unl: 1\n",
            "Ardh: 1\n",
            "Satya: 1\n",
            "finest: 1\n",
            "ever: 4\n",
            "Indian: 1\n",
            "Cinema: 1\n",
            "Directed: 1\n",
            "great: 2\n",
            "director: 1\n",
            "Go: 1\n",
            "My: 2\n",
            "exposure: 1\n",
            "Templarios: 1\n",
            "excited: 1\n",
            "find: 1\n",
            "title: 2\n",
            "among: 1\n",
            "off: 1\n",
            "significant: 1\n",
            "quotes: 1\n",
            "from: 3\n",
            "entire: 2\n",
            "pronounced: 1\n",
            "halfway: 1\n",
            "through: 3\n",
            "protago: 1\n",
            "expecting: 1\n",
            "much: 3\n",
            "got: 2\n",
            "pack: 1\n",
            "5: 1\n",
            "which: 4\n",
            "were: 5\n",
            "pret: 1\n",
            "bought: 1\n",
            "Blockbuster: 1\n",
            "$: 1\n",
            "3.00: 1\n",
            "because: 2\n",
            "sounded: 1\n",
            "interesting: 2\n",
            "bit: 1\n",
            "Ranma-esque: 1\n",
            "plot: 4\n",
            "death: 1\n",
            "children: 1\n",
            "Hopper: 1\n",
            "investigate: 1\n",
            "killing: 1\n",
            "Ever: 1\n",
            "lost: 1\n",
            "?: 2\n",
            "Well: 2\n",
            "did: 3\n",
            "even: 2\n",
            "begin: 2\n",
            "with.: 1\n",
            "Okay: 1\n",
            "kind: 2\n",
            "takes: 1\n",
            "route: 1\n",
            "'here: 1\n",
            "we: 3\n",
            "go: 1\n",
            "Week: 1\n",
            "week: 1\n",
            "David: 1\n",
            "Morse: 1\n",
            "pile: 1\n",
            "dung: 1\n",
            "husband: 2\n",
            "wondered: 1\n",
            "whether: 1\n",
            "actually: 2\n",
            "produ: 1\n",
            "clichés: 1\n",
            "type: 1\n",
            "substance: 1\n",
            "went: 1\n",
            "nowhere: 1\n",
            "end: 1\n",
            "book: 1\n",
            "Many: 1\n",
            "Splendored: 1\n",
            "Thing: 1\n",
            "Han: 1\n",
            "Suyin: 1\n",
            "tackles: 1\n",
            "issues: 1\n",
            "race: 1\n",
            "r: 2\n",
            "Of: 2\n",
            "seen: 8\n",
            "Rage: 1\n",
            "yet: 1\n",
            "direction: 1\n",
            "heard: 1\n",
            "States: 1\n",
            "Grace: 1\n",
            "open: 1\n",
            "God: 2\n",
            "struck: 1\n",
            "home: 1\n",
            "me: 3\n",
            "Being: 1\n",
            "29: 1\n",
            "'80: 1\n",
            "father: 1\n",
            "working: 1\n",
            "factory: 1\n",
            "As: 2\n",
            "disclaimer: 1\n",
            "'ve: 4\n",
            "5-6: 1\n",
            "last: 1\n",
            "15: 1\n",
            "years: 3\n",
            "musical: 1\n",
            "Protocol: 1\n",
            "implausible: 1\n",
            "whose: 1\n",
            "saving: 1\n",
            "grace: 1\n",
            "stars: 1\n",
            "Goldie: 1\n",
            "Hawn: 1\n",
            "along: 2\n",
            "g: 1\n",
            "How: 1\n",
            "could: 3\n",
            "classified: 1\n",
            "Drama: 1\n",
            "John: 1\n",
            "Voight: 1\n",
            "Mary: 1\n",
            "Steenburg: 1\n",
            "Preston: 1\n",
            "Sturgis: 1\n",
            "THE: 2\n",
            "POWER: 1\n",
            "AND: 1\n",
            "GLORY: 1\n",
            "unseen: 1\n",
            "public: 1\n",
            "nearly: 1\n",
            "twenty: 1\n",
            "thirty: 1\n",
            "Average: 1\n",
            "surprisingly: 1\n",
            "tame: 1\n",
            "Fulci: 2\n",
            "giallo: 1\n",
            "means: 2\n",
            "still: 3\n",
            "quite: 1\n",
            "normal: 1\n",
            "standards: 1\n",
            "b: 4\n",
            "Return: 1\n",
            "36th: 1\n",
            "Chamber: 1\n",
            "classic: 1\n",
            "Kung-Fu: 1\n",
            "Shaw: 1\n",
            "produces: 1\n",
            "back: 1\n",
            "*: 6\n",
            "SPOILERS: 2\n",
            "well: 4\n",
            "familiar: 1\n",
            "happens: 1\n",
            "Bela: 1\n",
            "Lugosi: 1\n",
            "appeared: 1\n",
            "several: 1\n",
            "these: 2\n",
            "low: 1\n",
            "budget: 1\n",
            "chillers: 1\n",
            "Monogram: 1\n",
            "Studios: 1\n",
            "1940: 1\n",
            "can: 2\n",
            "believe: 1\n",
            "anthology: 1\n",
            "stories: 1\n",
            "better: 2\n",
            "than: 1\n",
            "th: 1\n",
            "33: 1\n",
            "percent: 1\n",
            "nations: 1\n",
            "nitwits: 1\n",
            "support: 1\n",
            "W.: 1\n",
            "Bush: 1\n",
            "do: 3\n",
            "wh: 1\n",
            "someone: 1\n",
            "already: 2\n",
            "board: 1\n",
            "difficult: 1\n",
            "make: 2\n",
            "fake: 1\n",
            "documentary: 1\n",
            "Hills: 1\n",
            "Have: 1\n",
            "Eyes: 1\n",
            "II: 1\n",
            "what: 2\n",
            "expect: 1\n",
            "nothing: 1\n",
            "course: 1\n",
            "going: 2\n",
            "laughed: 1\n",
            "rotten: 1\n",
            "unbelievable: 1\n",
            "woman: 3\n",
            "leaves: 1\n",
            "her: 3\n",
            "af: 1\n",
            "NO: 1\n",
            "Hitchcock: 1\n",
            "successful: 2\n",
            "American: 1\n",
            "Rebecca: 1\n",
            "upon: 1\n",
            "Daphne: 1\n",
            "Dresser: 1\n",
            "evening: 1\n",
            "before: 1\n",
            "dozen: 1\n",
            "ago.: 1\n",
            "happened: 1\n",
            "basically: 1\n",
            "solid: 1\n",
            "plausible: 1\n",
            "premise: 1\n",
            "decent: 1\n",
            "ta: 1\n",
            "Fingersmith: 1\n",
            "'m: 2\n",
            "stunned: 1\n",
            "8/10: 1\n",
            "average: 1\n",
            "rating: 1\n",
            "show.: 1\n",
            "Cronenberg: 1\n",
            "usually: 1\n",
            "Besides: 1\n",
            "being: 1\n",
            "boring: 1\n",
            "oppressive: 1\n",
            "portray: 1\n",
            "m: 1\n",
            "unmarried: 1\n",
            "named: 1\n",
            "Stella: 1\n",
            "Bette: 1\n",
            "Midler: 1\n",
            "gets: 1\n",
            "pregnant: 1\n",
            "wealthy: 1\n",
            "man: 1\n",
            "Stephen: 2\n",
            "Collins: 1\n",
            "He: 2\n",
            "DO: 1\n",
            "N'T: 1\n",
            "TORTURE: 1\n",
            "DUCKLING: 1\n",
            "earlier: 1\n",
            "honestly: 1\n",
            "terms: 1\n",
            "story-line: 1\n",
            "Busty: 1\n",
            "beauty: 1\n",
            "Stacie: 1\n",
            "Randall: 1\n",
            "plays: 1\n",
            "PVC: 1\n",
            "clad: 1\n",
            "bad-ass: 1\n",
            "bitch: 1\n",
            "Alexandra: 1\n",
            "faithful: 1\n",
            "acolyte: 1\n",
            "Faust: 1\n",
            "Salman: 1\n",
            "Kahn: 1\n",
            "disappointed: 1\n",
            "o: 1\n",
            "why: 2\n",
            "producers: 1\n",
            "needed: 1\n",
            "trade: 1\n",
            "name: 1\n",
            "somewhat: 1\n",
            "franchise: 1\n",
            "laboured: 1\n",
            "predictable: 1\n",
            "lines: 1\n",
            "shallow: 1\n",
            "characters: 1\n",
            "s: 1\n",
            "Caddyshack: 1\n",
            "Two: 1\n",
            "compared: 1\n",
            "cant: 1\n",
            "stack: 1\n",
            "Robert: 1\n",
            "Stack: 1\n",
            "Honestly: 1\n",
            "-: 1\n",
            "short: 1\n",
            "sucks: 1\n",
            "dummy: 1\n",
            "used: 2\n",
            "necro: 1\n",
            "pretty: 1\n",
            "ph: 1\n",
            "Mukhsin: 1\n",
            "wonderfully: 1\n",
            "written: 1\n",
            "Its: 1\n",
            "entertainment: 1\n",
            "tonne: 1\n",
            "am: 1\n",
            "golf: 1\n",
            "any: 1\n",
            "On: 1\n",
            "May: 1\n",
            "26: 1\n",
            "10:30: 1\n",
            "PM: 1\n",
            "started: 1\n",
            "lat: 1\n",
            "Upon: 1\n",
            "viewing: 1\n",
            "Tobe: 1\n",
            "Hooper: 1\n",
            "gem: 1\n",
            "Crocodile: 1\n",
            "2000: 1\n",
            "developed: 1\n",
            "interest: 1\n",
            "college/croc: 1\n",
            "tries: 1\n",
            "epic: 1\n",
            "adventure: 1\n",
            "century: 1\n",
            "And: 1\n",
            "Shô: 1\n",
            "Kasugi: 1\n",
            "Christopher: 1\n",
            "Lee: 1\n",
            "Last: 1\n",
            "Men: 1\n",
            "finds: 1\n",
            "James: 1\n",
            "Coburn: 1\n",
            "outlaw: 1\n",
            "doing: 1\n",
            "long: 2\n",
            "sentence: 1\n",
            "breaking: 1\n",
            "free: 1\n",
            "chain: 1\n",
            "gang: 1\n",
            "Maybe: 1\n",
            "trailer: 1\n",
            "certainly: 1\n",
            "interview: 1\n",
            "DVD: 1\n",
            "di: 1\n",
            "attempt: 1\n",
            "depiction: 1\n",
            "revolutionary: 1\n",
            "struggle: 1\n",
            "resemble: 1\n",
            "hirsute: 1\n",
            "Boy: 1\n",
            "Sc: 1\n",
            "took: 1\n",
            "habit: 1\n",
            "finding: 1\n",
            "possible: 1\n",
            "Hawkings: 1\n",
            "genius: 1\n",
            "king: 1\n",
            "geniuses: 1\n",
            "Watching: 1\n",
            "makes: 2\n",
            "feel: 1\n",
            "dumb: 1\n",
            "Bu: 1\n",
            "psychic: 1\n",
            "Tory: 1\n",
            "returns: 1\n",
            "hometown: 1\n",
            "begins: 1\n",
            "reliving: 1\n",
            "trauma: 1\n",
            "Oh: 2\n",
            "11: 1\n",
            "twelve: 1\n",
            "ask: 1\n",
            "how: 2\n",
            "may: 1\n",
            "Fate: 1\n",
            "leads: 1\n",
            "Walter: 1\n",
            "Sparrow: 1\n",
            "come: 1\n",
            "possession: 1\n",
            "mysterious: 1\n",
            "novel: 2\n",
            "eerie: 1\n",
            "similarities: 1\n",
            "We: 1\n",
            "brought: 1\n",
            "joke: 2\n",
            "friend: 1\n",
            "our: 2\n",
            "play: 1\n",
            "probably: 2\n",
            "stupid: 1\n",
            "typical: 1\n",
            "Steele: 1\n",
            "two: 2\n",
            "people: 1\n",
            "undergone: 1\n",
            "sort: 1\n",
            "tragedy: 1\n",
            "noes: 1\n",
            "attack: 1\n",
            "Japanese: 1\n",
            "ghost: 1\n",
            "girl: 1\n",
            "Nicholas: 1\n",
            "Walker: 1\n",
            "Paul: 1\n",
            "local: 1\n",
            "town: 1\n",
            "Reverand: 1\n",
            "married: 1\n",
            "Martha: 1\n",
            "Ally: 1\n",
            "Sheedy: 1\n",
            "industry: 1\n",
            "laziest: 1\n",
            "world: 1\n",
            "needs: 1\n",
            "single: 1\n",
            "hit: 1\n",
            "flood: 1\n",
            "Down: 1\n",
            "Periscope: 1\n",
            "library: 1\n",
            "since: 1\n",
            "arrived: 1\n",
            "VHS: 1\n",
            "Since: 1\n",
            "acquired: 1\n",
            "curious: 1\n",
            "others: 1\n",
            "say: 1\n",
            "Deanna: 1\n",
            "Durbin: 1\n",
            "Nan: 1\n",
            "Grey: 1\n",
            "Barbara: 1\n",
            "Read: 1\n",
            "Three: 1\n",
            "Smart: 1\n",
            "Girls: 1\n",
            "Universal: 1\n",
            "1936: 1\n",
            "w: 1\n",
            "anyone: 1\n",
            "wondering: 1\n",
            "conversation: 1\n",
            "character: 1\n",
            "curiosity: 1\n",
            "wanting: 1\n",
            "possibly: 1\n",
            "ALL: 1\n",
            "moder: 1\n",
            "Daniel: 1\n",
            "Day-Lewis: 1\n",
            "versatile: 1\n",
            "actor: 1\n",
            "alive: 1\n",
            "English: 1\n",
            "aristocratic: 1\n",
            "snob: 1\n",
            "Room: 1\n",
            "With: 1\n",
            "View: 1\n",
            "guess: 1\n",
            "originally: 1\n",
            "parts: 1\n",
            "thus: 1\n",
            "quarter: 1\n",
            "lo: 1\n",
            "horror: 1\n",
            "B-Movies: 1\n",
            "cause: 1\n",
            "think: 1\n",
            "stupidity: 1\n",
            "unabi: 1\n",
            "IS: 1\n",
            "EVER: 1\n",
            "Mario: 1\n",
            "fond: 1\n",
            "memories: 1\n",
            "playing: 1\n",
            "Super: 1\n",
            "Ma: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords  #stop word method\n",
        "#without tokenizarion\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def word_frequency_without_tokenization(file_path, target_word):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    lines = text_lower.split('\\n')\n",
        "\n",
        "    frequency_distribution = FreqDist()\n",
        "\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "\n",
        "        frequency_distribution.update([word for word in words if word == target_word.lower()])\n",
        "\n",
        "    return frequency_distribution[target_word.lower()]\n",
        "\n",
        "file_path = 'text.txt'\n",
        "word_to_find = 'good'\n",
        "freq = word_frequency_without_tokenization(file_path, word_to_find)\n",
        "print(f\"The word '{word_to_find}' appears {freq} times in the text.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcITebjzgxs6",
        "outputId": "c5d91c4c-0a05-4051-daa5-27f1fa1d06e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'good' appears 3 times in the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to read and preprocess text from a file\n",
        "def preprocess_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Tokenize words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Convert tokens to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Example file paths\n",
        "file_paths = ['text1.txt', 'text2.txt']\n",
        "# Labels for each file\n",
        "labels = ['class1', 'class2']\n",
        "\n",
        "# Preprocess texts and create corpus\n",
        "corpus = [preprocess_text(file_path) for file_path in file_paths]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Example prediction using SVM\n",
        "example_text = preprocess_text('example_text.txt')\n",
        "example_text_vectorized = vectorizer.transform([example_text])\n",
        "svm_prediction = svm_classifier.predict(example_text_vectorized)\n",
        "print(\"SVM Prediction:\", svm_prediction[0])\n",
        "\n",
        "# Example prediction using Random Forest\n",
        "rf_prediction = rf_classifier.predict(example_text_vectorized)\n",
        "print(\"Random Forest Prediction:\", rf_prediction[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "H_w8svhDvUYc",
        "outputId": "a6464c6f-8918-4faa-dcaa-5d31a1bfedf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The number of classes has to be greater than one; got 1 class",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c43c67779b89>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# SVM Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0msvm_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0msvm_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Random Forest Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         sample_weight = np.asarray(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    750\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
          ]
        }
      ]
    }
  ]
}
